{    
    "Mode"                             	: "train",
    "Mode_comment"                     	: "select from {train, test, continued_train} which are class Mode's names",
    "Environment"                       : "CartPole-v1",
    "Agent"                             : "DQN",
    "LogPath"                           : "./log",
    "dtype"                            	: "float32",
    "intdtype"                          : "int32",

    "XXNetUnits_comment"                : "units of hidden layers like [64,'bn',64]. 'bn' from BatchNormInUnitsList", 
    "BatchNorm_inUnitsList"             : "bn",
    "DQN_hiddenUnits"               	: [32, 32],
    "Actor_hiddenUnits"                 : [32, 32],
    "Critic_hiddenUnits"                : [32, 64, 32],
    "Critic_observationBlock_hiddenUnits"   : [32, 32],
    "Critic_actionBlock_hiddenUnits"        : [32, 32],
    "Critic_concatenateBlock_hiddenUnits"   : [128, 16],

    "SavePath"                          : "./model",
    "SummaryWriterPath"                 : "./summarywriter",
    "NumOfEpisodes_toTrain"             : 10000, 
    "NumOfEpisodes_train_comment"       : "10000 for cartpoleV1", 
    "#  SumReward_toStopTrain"          : 200,
    "#  #   SumReward_toStopTrain"      : "for cartpoleV1.",
    "#  AvgReward_toStopTrain"          : -0.005,
    "#  #   AvgReward_toStopTrain"      : "-0.001 for pendulumV1",
    "NumOfEpisodes_toTest"              : 3,
    "Period_toSaveModels"               : 100,
    "#  Period_toSaveModels"            : "100 for cartpoleV1, in number of episodes",
    "BatchSize"                         : 64, 
    "DQN_learningRate"                  : 0.00025,
    "Actor_learningRate"                : 0.0003,
    "#  Actor_learningRate"             : "0.0003 in general",
    "Critic_learningRate"               : 0.0006,
    "#  Critic_learningRate"            : "0.0006 in general",

    "TargetActor"                       : false,
    "Critic2"                           : true,
    "RewardNormalization"               : false,
    "PER"                               : false,
    "MemoryCapacity"                    : 10000,
    "MemoryRatio_toStartTrain"          : 0.001, 
    "#  MemoryRatio_toStartTrain"       : "0.5 for cartpoleV1, start train after filling replay buffer with random actions",
    "MemoryRatio_toFillWithRandomAction": 0.5, 
    "SoftUpdateRate_tau"                : 0.003, 
    "RewardDiscountRate_gamma"          : 0.99,
    "EpsilonInit"                       : 1.0,
    "EpsilonDecay"                      : 0.99,
    "EpsilonMin"                        : 0.01,
    "EpsilonMin_comment"                : "0.01 in general",
    "EpsilonLambda"                     : 0.0001,
    "TemperatureParameter_alpha"        : 0.06,
    "#  TemperatureParameter_alpha"     : "0.06 for SAC, SAC_discrete, 5.0 for SAC_entropy",
    "SAC_entropy_isActionStochastic"    : false,

    "CartPole_v1"                       : { 
        "observ": {
            "nNodes": [1,1,1,1],
            "low": [-4.8, -1e+38, -0.418, -1e+38],
            "high": [4.8, 1e+38, 0.418, 1e+38],
            "possibles": [],
            "scaleshift": null, 
            "isDecodedScalar": false
        },
        "action": {
            "nNodes": [2],
            "low": [],
            "high": [],
            "possibles": [[0,1]],
            "scaleshift": null, 
            "isDecodedScalar": true
        },
        "targetToMonitor": "sumReward", 
        "sumReward_toStopTrain" : 200, 
        "avgReward_toStopTrain" : 0
    },
    "Pendulum_v1"                       : { 
        "observ": {
            "nNodes": [1,1,1],
            "low": [-1, -1, -8],
            "high": [1,1,8],
            "possibles": [],
            "scaleshift": null, 
            "isDecodedScalar": false
        },
        "action": {
            "nNodes": [1],
            "low": [-2],
            "high": [2],
            "possibles": [],
            "scaleshift": null, 
            "isDecodedScalar": false
        },
        "targetToMonitor": "avgReward", 
        "sumReward_toStopTrain" : 0,
        "avgReward_toStopTrain" : -0.005
    },
    "LunarLander_v2"                    : { 
        "observ": {
            "nNodes": [1,1,1,1,1,1,1,1],
            "low": [-90,-90,-5,-5,-3.1415927,-5,-0,-0], 
            "high": [90,90,5,5,3.1415927,5,1,1],
            "possibles": [],
            "scaleshift": null, 
            "isDecodedScalar": false
        },
        "action": {
            "nNodes": [4],
            "low": [],
            "high": [],
            "possibles": [[0,1,2,3]],
            "scaleshift": null, 
            "isDecodedScalar": true
        },
        "targetToMonitor": "sumReward", 
        "sumReward_toStopTrain" : 200,
        "avgReward_toStopTrain" : 0
    },

    "#  ending"                         : true
}
